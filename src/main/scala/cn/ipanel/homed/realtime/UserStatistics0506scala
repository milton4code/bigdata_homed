package cn.ipanel.homed.realtime

import java.util

import cn.ipanel.common.{Constant, DBProperties, SparkSession, Tables}
import cn.ipanel.etl.LogConstant
import cn.ipanel.utils.{DBUtils, DateUtils, PropertiUtils, RedisClient}
import com.alibaba.fastjson.{JSON, JSONObject}
import com.examples.Log4jPrintStream
import kafka.serializer.StringDecoder
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{DataFrame, Row, SQLContext}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Durations, StreamingContext}
import redis.clients.jedis.ScanParams.SCAN_POINTER_START
import redis.clients.jedis.{ScanParams, ScanResult}

/**
  * 用户统计
  * 主要统计开机用户、直播、点播、回看在线用户数
  */

case class Log3(DA: Long, channel_id: Long, device_type: String, device_id: String,
                report_time: Long, receive_time: Long, keyword: String,
                province_id: String, city_id: String, region_id: String, play_type: String,
                status: Int, play_time: Int)

private[this] case class CachUser2(device_id: Long = 0L, DA: Long = 0L, play_type: String = "", device_type: String = "", province_id: String = "", city_id: String = "", region_id: String = "",
                                   report_time: Long = 0L, receive_time: Long = 0L)

object UserStatistics2 {
  private lazy val sqlPro = PropertiUtils.init("sql/user_statistics.properties")
  private lazy val checkpointPath = "/spark/checkpoint/userstatistics"

  //省市县|终端类型|观看类型
  //  val run_log_topic = "run_log_topic"
  //------------------临时表---------------
  // 用户在线临时表
  private lazy val T_USER_ONLINE = "t_tmp_user_online"
  //redis缓存临时表
  private lazy val T_CACHE = "t_cache"
  /** 过期时间 */
  val EXPIRE_TIME = 30
  /** redis 库索引 */
  val DB_INDEX = 13

  def main(args: Array[String]): Unit = {
    Log4jPrintStream.redirectSystemOut()
    //run_log_topic my_group_id112 city 20 4  30 0

    val run_log_topic: String = args(0)
    val group_id: String = args(1)
    val area_type: String = args(2)
    val duration: Long = args(3).toLong
    val threads: Int = args(4).toInt
    val expire_time: Int = args(5).toInt
    val play_time: Int = args(6).toInt

    val sparkSession = SparkSession("UserStatistics", "")
    val sparkContext = sparkSession.sparkContext


    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.apache.hadoop").setLevel(Level.ERROR)
    Logger.getLogger("org.apache.zookeeper").setLevel(Level.WARN)
    Logger.getLogger("org.apache.hive").setLevel(Level.WARN)


    val run_log_topics = Set(run_log_topic)
    val brokers = "192.168.18.60:9092,192.168.18.61:9092,192.168.18.63:9092"
    val kafkaParams = Map[String, String]("metadata.broker.list" -> brokers,
      "serializer.class" -> "kafka.serializer.StringEncoder", "group.id" -> group_id, "auto.offset.reset" -> "largest")

    //确保在kill任务时，能够处理完最后一批数据，再关闭程序，不会发生强制kill导致数据处理中断，没处理完的数据丢失
    //    sparkContext.getConf.set("spark.streaming.stopGracefullyOnShutdown", "true")
    //开启后spark自动根据系统负载选择最优消费速率,
    //    sparkContext.getConf.set("spark.streaming.backpressure.enabled", "true")
    //注册序列化类
    sparkContext.getConf.registerKryoClasses(Array(classOf[CachUser2], classOf[Log3]))

    val ssc = new StreamingContext(sparkSession.sparkContext, Durations.seconds(duration))
    val runlogSream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, run_log_topics)
    val runLogDStream: DStream[Log3] = filterLog(runlogSream)

    foreachRdd(runLogDStream, threads, area_type, play_time, expire_time)


    ssc.start()
    //延长数据在程序中的生命周期
    ssc.remember(Durations.seconds(duration * 5))
    ssc.awaitTermination()

  }


  def foreachRdd(runLogDStream: DStream[Log3], threads: Int, area_type: String, play_time: Int, expire_time: Int): Unit = {
    runLogDStream.foreachRDD(event => {
      val sqlContext: SQLContext = SQLContext.getOrCreate(event.sparkContext)
      import sqlContext.implicits._

      val logDF = event.toDF()
      logDF.registerTempTable("t_log")


      //加载缓存数据
      val preCacheUsersDF: DataFrame = loadPreviousUserStatus(sqlContext)
      preCacheUsersDF.registerTempTable(T_CACHE)
      //加载在线用户数据
      val userOnlineDF: DataFrame = loadUserOnlineStatus(sqlContext)
      userOnlineDF.registerTempTable(T_USER_ONLINE)

      //根据用户上报数据分组,得到用户播放类型的开始和结束状态,
      //主要有3种情况 1.开始1 结束0 ,则说明用户结束了该次播放 ;2.开始1,结束空,则说明用户正在播放; 3.开始空,结束0,则说明用户结束上次播放
      val logGroupDF = sqlContext.sql(sqlPro.getProperty("log_group")).filter($"play_time" >= play_time)
      logGroupDF.registerTempTable("t_log_group")
      logGroupDF.persist(StorageLevel.MEMORY_ONLY_SER)

      //      logGroupDF.selectExpr("da as log_groupDA", "play_type", "start_status", "end_status").show(1111)

      //获取用户多次切台数据,保留每种播放行为最后一次记录
      val dropDuplicateDF1 = sqlContext.sql(sqlPro.getProperty("drop_duplicate1")) //不用缓存,但需要计算
      //新一轮播放开始
      val dropDuplicateDF2 = sqlContext.sql(sqlPro.getProperty("drop_duplicate2")) //要缓存,切要计算

      dropDuplicateDF1.unionAll(dropDuplicateDF2).registerTempTable("t_drop_union")
      //连续切换场景数据去重,取每种播放类型最后一次播放请求
      val dropDuplicateDF4 = sqlContext.sql(sqlPro.getProperty("drop_duplicate4").replace("@table@", "t_drop_union"))

      //dropDuplicateDF4.selectExpr("DA as lastLog_da", "start_status", "end_status", "play_type", "from_unixtime(report_time) as report_time", "from_unixtime(receive_time) as receive_time").show(1000, false)

      if (preCacheUsersDF.rdd.isEmpty()) { //缓存数据为空，则说明程序刚启动
        println("====缓存数据为空==" + DateUtils.getNowDate(DateUtils.YYYY_MM_DD_HHMMSS))

        process(dropDuplicateDF4, sqlContext, threads, area_type)
        cacheUserToRedis(dropDuplicateDF4, expire_time)
      } else {
        //上一次播放结束
        sqlContext.sql(sqlPro.getProperty("drop_duplicate3")).registerTempTable("t_drop_duplicate3")
        //确定缓存终端数据场景是否切换,主要是剔除上个场景已结束的数据
        val activeUserDF2 = sqlContext.sql(sqlPro.getProperty("active_user_compare"))
        //        activeUserDF2.selectExpr("DA as compare_DA", "play_type", "device_type", "from_unixtime(report_time) as report_time", "from_unixtime(receive_time) as receive_time")
        //          .show(1000, false)

        //通过缓存数据和日志上报数据再次确定用户最后一次的播放行为
        activeUserDF2.unionAll(dropDuplicateDF4).registerTempTable("t_union_2")
        val df = sqlContext.sql(sqlPro.getProperty("drop_duplicate4").replace("@table@", "t_union_2"))
        process(df, sqlContext, threads, area_type)
        cacheUserToRedis(df, expire_time)

        //         df.selectExpr("DA as union_Da", "play_type", "device_type", "from_unixtime(report_time) as report_time", "from_unixtime(receive_time) as receive_time")
        //           .show(1000, false)
      }

      logDF.unpersist()
      logGroupDF.unpersist()
    })

  }

  /**
    * 加载前一次用户状态
    *
    * @return device_id|DA|playType|device_type|DA|province_id|city_id|region_id|report_time|receive_time
    */
  private def loadPreviousUserStatus(sqlContext: SQLContext): DataFrame = {
    val start = System.currentTimeMillis()
    val redisResponse = loadUserStatusFromReids()
    val end = System.currentTimeMillis()
    println("load from redis cost " + (end - start) + "ms")
    redisToDataFrame(redisResponse, sqlContext)
  }

  /**
    * redis数据转成DataFrame
    *
    * @return device_id|DA|playType|device_type|DA|province_id|city_id|region_id|report_time|receive_time
    */
  private def redisToDataFrame(redisResponse: Map[String, String], sqlContext: SQLContext): DataFrame = {
    val buffer = new util.ArrayList[Row]
    println("redisResponse==isEmpty=" + redisResponse.isEmpty)

    redisResponse.foreach(x => {
      val device_id = x._1.split("\\" + Constant.REDIS_SPILT)(0)
      val DA = x._1.split("\\" + Constant.REDIS_SPILT)(1).toLong
      val play_type = x._1.split("\\" + Constant.REDIS_SPILT)(2)
      //      println("======paras===" + x._2)
      val paras = x._2.split("\\" + Constant.REDIS_SPILT)
      buffer.add(Row(device_id, DA, play_type, paras(0), paras(1), paras(2), paras(3),
        paras(4).toLong, paras(5).toLong, paras(6).toInt, paras(7).toInt))
    })
    println("redisToDataFrame buffer is :" + buffer.isEmpty)
    sqlContext.createDataFrame(buffer, getCacheUserSchema())
  }

  def getCacheUserSchema(): StructType = {
    import org.apache.spark.sql.types._
    StructType(List(
      StructField("device_id", StringType, nullable = false),
      StructField("DA", LongType, nullable = false),
      StructField("play_type", StringType, nullable = false),
      StructField("device_type", StringType, nullable = true),
      StructField("province_id", StringType, nullable = true),
      StructField("city_id", StringType, nullable = true),
      StructField("region_id", StringType, nullable = true),
      StructField("report_time", LongType, nullable = true),
      StructField("receive_time", LongType, nullable = true),
      StructField("start_status", IntegerType, nullable = true),
      StructField("end_status", IntegerType, nullable = true)
    ))
  }

  /**
    * 从redis中加载用户状态
    */
  private def loadUserStatusFromReids(): Map[String, String] = {
    var responses = Map[String, String]()
    val jedis = RedisClient.getRedis()
    jedis.select(DB_INDEX)

    val scanParams: ScanParams = new ScanParams().count(1000).`match`("*")
    var cur: String = SCAN_POINTER_START

    // scala遍历java集合时需要导入此类
    import scala.collection.JavaConversions._
    do {
      val scanResult: ScanResult[String] = jedis.scan(cur, scanParams)
      for (key: String <- scanResult.getResult) {
        responses += (key -> jedis.get(key))
      }
      cur = scanResult.getStringCursor
    } while (!(cur == SCAN_POINTER_START))
    RedisClient.release(jedis)

    responses
  }

  /**
    * 加载用户在线状态
    *
    * @return DA|device_id|device_type|ipaddress|login_time|province_id|city_id|region_id
    */
  private def loadUserOnlineStatus(sqlContext: SQLContext): DataFrame = {
    val userStatusTable =
      s"""
         |( SELECT f_user_id as DA,f_device_id as device_id,case f_device_type
         |WHEN 1 then "stb" when 2 then "smartcard"
         |when 3 then "mobile" when 4 then "pad"
         |when 5 then "pc"
         |ELSE "unknown" end device_type,
         |CONCAT(SUBSTR(f_region_id,1,2),"0000") province_id,
         |CONCAT(SUBSTR(f_region_id,1,4),"00") city_id,
         |SUBSTR(f_region_id,1,6) region_id
         |from t_user_online_statistics  ) as user
         """.stripMargin

    val prop = new java.util.Properties
    prop.setProperty("user", DBProperties.USER_IACS)
    prop.setProperty("password", DBProperties.PASSWORD_IACS)

    sqlContext.read.jdbc(DBProperties.JDBC_URL_IACS, userStatusTable, "DA", 1, 99999999, 5, prop)
  }

  def process(df: DataFrame, sqlContext: SQLContext, threads: Int, area_type: String) = {
    df.registerTempTable("t_tmp")
    area_type.toLowerCase() match {
      case Constant.PROVINCE => statisticsByProvice(df, sqlContext, threads)
      case Constant.CITY => statisticsByCity(df, sqlContext, threads)
    }
  }

  /**
    * 统计省网在线用户
    */
  def statisticsByProvice(eventDF: DataFrame, sqlContext: SQLContext, threads: Int) = {

    import sqlContext.sql
    //1、按省 终端类型|收看类型 统计用户数量
    val provice_singlg_deviceTypeDF = sql(sqlPro.getProperty("provice_single_deviceType"))
    //2、按省 市 终端类型|收看类型（单）
    val provice_city_single_deviceTypeDF = sql(sqlPro.getProperty("provice_city_single_deviceType"))
    //3、按省 全部终端类型|收看类型（总）
    val all_provice_all_deviceTypeDF = sql(sqlPro.getProperty("all_provice_all_deviceType"))
    //4、按省 市 全部终端类型|收看类型（总）
    val provice_city_all_deviceTypeDF = sql(sqlPro.getProperty("provice_city_all_deviceType"))

    val resultDF = provice_singlg_deviceTypeDF.unionAll(provice_city_single_deviceTypeDF)
      .unionAll(all_provice_all_deviceTypeDF).unionAll(provice_city_all_deviceTypeDF)
  }

  /**
    * 统计市网在线用户
    */
  def statisticsByCity(df: DataFrame, sqlContext: SQLContext, threads: Int) = {

    import sqlContext.implicits._
    //    eventDF.selectExpr("da as event_da", "Da", "play_type").show()
    val result = df.mapPartitions(x => {
      //      var res = ListBuffer[(String, Int)]()
      var res = List[(String, Int)]()
      while (x.hasNext) {
        val row = x.next()
        //        val device_id = row.getAs[Long]("device_id")
        val device_type = row.getAs[String]("device_type")
        //        val DA = row.getAs[Long]("DA")
        val play_type = row.getAs[String]("play_type")

        val province_id = row.getAs[String]("province_id")
        val city_id = row.getAs[String]("city_id")
        val region_id = row.getAs[String]("region_id")

        //6 按市 终端类型|收看类型（单）
        res = res.::(("c1," + province_id + "," + city_id + "," + device_type + "," + play_type, 1))
        //7、按市|区 终端类型|收看类型（单）
        res = res.::(("c2," + province_id + "," + city_id + "," + region_id + "," + device_type + "," + play_type, 1))
        //8、按市 全部终端类型|收看类型（总）
        res = res.::(("c3," + province_id + "," + city_id + "," + play_type, 1))
        //9、按市 区 全部终端类型|收看类型（总）
        res = res.::(("c4," + province_id + "," + city_id + "," + region_id + "," + play_type, 1))
        //10、按市　全部终端　全部收看类型 总
        res = res.::(("c5," + province_id + "," + city_id, 1))

      }
      res.toIterator
    }).reduceByKey((v1: Int, v2: Int) => v1 + v2, threads)
      .map { case (group, count) =>
        var reuslt = Result()
        val group_fileds = group.split(",")
        group_fileds(0) match {
          case "c1" =>
            reuslt = reuslt.copy(group_fileds(1), group_fileds(2), "all", group_fileds(3), group_fileds(4), count)
          case "c2" =>
            reuslt = reuslt.copy(group_fileds(1), group_fileds(2), group_fileds(3), group_fileds(4), group_fileds(5), count)
          case "c3" =>
            reuslt = reuslt.copy(group_fileds(1), group_fileds(2), "all", "all", group_fileds(3), count)
          case "c4" =>
            reuslt = reuslt.copy(group_fileds(1), group_fileds(2), group_fileds(3), "all", group_fileds(4), count)
          case "c5" =>
            reuslt = reuslt.copy(group_fileds(1), group_fileds(2), "all", "all", "all", count)
          case _ =>
        }
        reuslt
      }.toDF().filter($"province_id" !== "")

    save(result)
  }

  def save(result: DataFrame): Unit = {
    val resultDF = result.selectExpr(s"'${DateUtils.getNowDate(DateUtils.YYYY_MM_DD_HHMMSS)}' as time",
      "province_id", "city_id", "region_id", "device_type", "device_type", "play_type", "user_count")
    DBUtils.saveDataFrameToPhoenixNew(resultDF, Tables.T_USER_ONLINE)
  }

  /**
    * 缓存数据到redis中
    * redis结构为 设备ID|DA|playType  x._2 结构为 device_type|DA|province_id|city_id|region_id|report_time|receive_time
    */
  private[this] def cacheUserToRedis(cacheDF: DataFrame, expire_time: Int): Unit = {
    if (!cacheDF.rdd.isEmpty()) {
      val start = System.currentTimeMillis()

      cacheDF.foreachPartition(it => {
        val jedis = RedisClient.getRedis()
        val pipeline = jedis.pipelined()
        pipeline.select(DB_INDEX)

        val redisValue = new StringBuilder()
        while (it.hasNext) {
          val log = it.next()
          val key = log.getAs[String]("device_id") +
            Constant.REDIS_SPILT + log.getAs[Long]("DA") +
            Constant.REDIS_SPILT + log.getAs[String]("play_type")

          redisValue
            .append(log.getAs[String]("device_type")).append(Constant.REDIS_SPILT)
            .append(log.getAs[String]("province_id")).append(Constant.REDIS_SPILT)
            .append(log.getAs[String]("city_id")).append(Constant.REDIS_SPILT)
            .append(log.getAs[String]("region_id")).append(Constant.REDIS_SPILT)
            .append(log.getAs[Long]("report_time")).append(Constant.REDIS_SPILT)
            .append(log.getAs[Long]("receive_time")).append(Constant.REDIS_SPILT)
            .append(log.getAs[Int]("start_status")).append(Constant.REDIS_SPILT)
            .append(log.getAs[Int]("end_status"))

          //          println("redis :: " + key + "===" + redisValue)
          //          pipeline.zadd(key, log.getAs[Long]("receive_time") + log.getAs[Long]("report_time"), redisValue.toString())
          pipeline.set(key, redisValue.toString())
          pipeline.expire(key, expire_time)
          redisValue.clear()
        }
        pipeline.sync()
        RedisClient.release(jedis)

      })
      val end = System.currentTimeMillis()
      println("cacheUserToRedis =-=-=-=--cost=" + (end - start) + " ms")
    }
  }

  //过滤有效日志
  def filterLog(kafkaStream: InputDStream[(String, String)]): DStream[Log3] = {
    kafkaStream.transform(rdd => {
      //      KafkaOffsetManager.saveOffsets(zkClient,zkHosts, zkPath, rdd)
      rdd.flatMap(line => {
        try {
          val data = JSON.parseObject(line._2)
          data.put("receive_time", System.currentTimeMillis() / 1000)
          Some(data)
        } catch {
          case ex: Exception =>
            Some(new JSONObject())
        }
      }).filter(x => { //过滤直播的日志
        val keyword = x.getString("keyword")
        keyword.equalsIgnoreCase(LogConstant.videoSuccess) || keyword.equalsIgnoreCase(LogConstant.videFinished)
      }).map(x => {
        //        println("data===filter===========" + x)
        val regionIdStr = TokenParser.getRegionString(x.getString("accesstoken"))
        val proviceId = regionIdStr.substring(0, 2) + "0000"
        val cityId = regionIdStr.substring(0, 4) + "00"
        val regionId = regionIdStr.substring(0, 6)
        val statuts = if (LogConstant.videoSuccess.equals(x.getString("keyword"))) LogConstant.SUCCESS else LogConstant.FINISH

        Log3(x.getLongValue("DA"), 0L,
          x.getString("DeviceType"), x.getString("DeviceID"),
          x.getLongValue("timeunix"), x.getLongValue("receive_time"),
          "", proviceId, cityId, regionId, x.getString("playtype"),
          statuts, x.getString("PlayS").toInt)
      }) /*.filter( "000000" != _.province_id )*/
      //        .filter(_.DA == 55313240)
    })
  }

  private[this] def cacheToMysql(cacheDF: DataFrame) {
    val cache = cacheDF.selectExpr(s"'${DateUtils.getNowDate(DateUtils.YYYY_MM_DD_HHMMSS)}' as time", " device_id", "DA", "play_type")
    //    DBUtils.saveDataFrameToPhoenixNew(cache,"t_cache_user")
    DBUtils.saveToHomedData_2(cache, "t_cache_user")
  }


}

